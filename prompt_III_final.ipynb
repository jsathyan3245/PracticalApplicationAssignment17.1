{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Application III: Comparing Classifiers\n",
    "\n",
    "**Overview**: In this practical application, your goal is to compare the performance of the classifiers we encountered in this section, namely K Nearest Neighbor, Logistic Regression, Decision Trees, and Support Vector Machines.  We will utilize a dataset related to marketing bank products over the telephone.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Our dataset comes from the UCI Machine Learning repository [link](https://archive.ics.uci.edu/ml/datasets/bank+marketing).  The data is from a Portugese banking institution and is a collection of the results of multiple marketing campaigns.  We will make use of the article accompanying the dataset [here](CRISP-DM-BANK.pdf) for more information on the data and features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Understanding the Data\n",
    "\n",
    "To gain a better understanding of the data, please read the information provided in the UCI link above, and examine the **Materials and Methods** section of the paper.  How many marketing campaigns does this data represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The dataset represents multiple marketing campaigns conducted by a Portuguese banking institution. Specifically, the data spans 17 distinct marketing campaigns aimed at promoting a term deposit product. Each campaign involved reaching out to bank clients via telephone and recording various socio-economic and campaign-specific details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Read in the Data\n",
    "\n",
    "Use pandas to read in the dataset `bank-additional-full.csv` and assign to a meaningful variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/bank-additional-full.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education  default housing loan    contact  \\\n",
       "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1   57   services  married  high.school  unknown      no   no  telephone   \n",
       "2   37   services  married  high.school       no     yes   no  telephone   \n",
       "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4   56   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week  ...  campaign  pdays  previous     poutcome emp.var.rate  \\\n",
       "0   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "1   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "2   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "3   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "4   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0          93.994          -36.4      4.857       5191.0  no  \n",
       "1          93.994          -36.4      4.857       5191.0  no  \n",
       "2          93.994          -36.4      4.857       5191.0  no  \n",
       "3          93.994          -36.4      4.857       5191.0  no  \n",
       "4          93.994          -36.4      4.857       5191.0  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Understanding the Features\n",
    "\n",
    "\n",
    "Examine the data description below, and determine if any of the features are missing values or need to be coerced to a different data type.\n",
    "\n",
    "\n",
    "```\n",
    "Input variables:\n",
    "# bank client data:\n",
    "1 - age (numeric)\n",
    "2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "# related with the last contact of the current campaign:\n",
    "8 - contact: contact communication type (categorical: 'cellular','telephone')\n",
    "9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "# other attributes:\n",
    "12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "# social and economic context attributes\n",
    "16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n",
    "18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n",
    "19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "Output variable (desired target):\n",
    "21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To understand the features in the dataset provided for the marketing campaign analysis, I will examine each feature based on the description given. This will help determine if there are any missing values or if any features need to be coerced to a different data type.\n",
    "\n",
    "Features Overview\n",
    "age (numeric)\n",
    "\n",
    "Type Check: This feature should be numeric.\n",
    "Missing Values: I will check for any NaN or null values in the age column.\n",
    "job (categorical)\n",
    "\n",
    "Type Check: This feature is categorical and should be converted to a categorical type if it is not already.\n",
    "Missing Values: I will look for any 'unknown' entries or NaN values that may indicate missing data.\n",
    "marital (categorical)\n",
    "\n",
    "Type Check: This feature is also categorical.\n",
    "Missing Values: I will check for 'unknown' entries or NaN values.\n",
    "education (categorical)\n",
    "\n",
    "Type Check: This feature is categorical.\n",
    "Missing Values: I will verify if there are any 'unknown' entries or NaN values.\n",
    "default (categorical)\n",
    "\n",
    "Type Check: This feature is categorical.\n",
    "Missing Values: I will check for any 'unknown' entries or NaN values.\n",
    "housing (categorical)\n",
    "\n",
    "Type Check: This feature is categorical.\n",
    "Missing Values: I will look for 'unknown' entries or NaN values.\n",
    "loan (categorical)\n",
    "\n",
    "Type Check: This feature is categorical.\n",
    "Missing Values: I will check for any 'unknown' entries or NaN values.\n",
    "contact (categorical)\n",
    "\n",
    "Type Check: This feature is categorical.\n",
    "Missing Values: I will verify for NaN values.\n",
    "month (categorical)\n",
    "\n",
    "Type Check: This feature is categorical.\n",
    "Missing Values: I will check for NaN values.\n",
    "day_of_week (categorical)\n",
    "\n",
    "Type Check: This feature is categorical.\n",
    "Missing Values: I will verify for NaN values.\n",
    "duration (numeric)\n",
    "\n",
    "Type Check: This feature should be numeric.\n",
    "Missing Values: I will check for NaN values.\n",
    "Note: I must be cautious, as this feature may not be used in a predictive model for realistic scenarios.\n",
    "campaign (numeric)\n",
    "\n",
    "Type Check: This feature should be numeric.\n",
    "Missing Values: I will check for NaN values.\n",
    "pdays (numeric)\n",
    "\n",
    "Type Check: This feature should be numeric.\n",
    "Missing Values: I will check for NaN values, noting that a value of 999 indicates no previous contact.\n",
    "previous (numeric)\n",
    "\n",
    "Type Check: This feature should be numeric.\n",
    "Missing Values: I will check for NaN values.\n",
    "poutcome (categorical)\n",
    "\n",
    "Type Check: This feature is categorical.\n",
    "Missing Values: I will verify for NaN values.\n",
    "emp.var.rate (numeric)\n",
    "\n",
    "Type Check: This feature should be numeric.\n",
    "Missing Values: I will check for NaN values.\n",
    "cons.price.idx (numeric)\n",
    "\n",
    "Type Check: This feature should be numeric.\n",
    "Missing Values: I will check for NaN values.\n",
    "cons.conf.idx (numeric)\n",
    "\n",
    "Type Check: This feature should be numeric.\n",
    "Missing Values: I will check for NaN values.\n",
    "euribor3m (numeric)\n",
    "\n",
    "Type Check: This feature should be numeric.\n",
    "Missing Values: I will check for NaN values.\n",
    "nr.employed (numeric)\n",
    "\n",
    "Type Check: This feature should be numeric.\n",
    "Missing Values: I will check for NaN values.\n",
    "y (binary)\n",
    "\n",
    "Type Check: This feature is categorical (binary).\n",
    "Missing Values: I will verify for NaN values.\n",
    "Steps for Data Preparation\n",
    "Check for Missing Values: For categorical variables, I will identify any 'unknown' values, as these may indicate missing data. I can use methods like .isnull().sum() or .value_counts() to find these.\n",
    "\n",
    "Data Type Coercion:\n",
    "\n",
    "I will convert categorical variables to the appropriate data type (e.g., using pd.Categorical in pandas).\n",
    "I will ensure numeric features are in float or int format as needed.\n",
    "Handle Unknowns: For categorical variables with 'unknown' values, I need to decide whether to:\n",
    "\n",
    "Replace them with the mode of the column.\n",
    "Drop those rows entirely, especially if the proportion is small.\n",
    "Duration Feature: Given that the duration should not be included in the predictive model for realistic predictions, I can either drop this feature or mark it for exclusion during model training.\n",
    "\n",
    "Final Checks: After addressing missing values and data types, I will conduct a final review of the data using descriptive statistics to ensure all features are correctly formatted and any anomalies are handled.\n",
    "\n",
    "Summary\n",
    "After completing these checks, I will be better positioned to prepare the dataset for analysis. Addressing missing values and ensuring proper data types are crucial for the performance of classification algorithms. If I need further assistance with any specific implementation steps or code, I can reach out for help!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Understanding the Task\n",
    "\n",
    "After examining the description and data, your goal now is to clearly state the *Business Objective* of the task.  State the objective below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             41188 non-null  int64  \n",
      " 1   job             41188 non-null  object \n",
      " 2   marital         41188 non-null  object \n",
      " 3   education       41188 non-null  object \n",
      " 4   default         41188 non-null  object \n",
      " 5   housing         41188 non-null  object \n",
      " 6   loan            41188 non-null  object \n",
      " 7   contact         41188 non-null  object \n",
      " 8   month           41188 non-null  object \n",
      " 9   day_of_week     41188 non-null  object \n",
      " 10  duration        41188 non-null  int64  \n",
      " 11  campaign        41188 non-null  int64  \n",
      " 12  pdays           41188 non-null  int64  \n",
      " 13  previous        41188 non-null  int64  \n",
      " 14  poutcome        41188 non-null  object \n",
      " 15  emp.var.rate    41188 non-null  float64\n",
      " 16  cons.price.idx  41188 non-null  float64\n",
      " 17  cons.conf.idx   41188 non-null  float64\n",
      " 18  euribor3m       41188 non-null  float64\n",
      " 19  nr.employed     41188 non-null  float64\n",
      " 20  y               41188 non-null  object \n",
      "dtypes: float64(5), int64(5), object(11)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Business Objective\n",
    "The primary objective of my analysis is to evaluate the effectiveness of marketing campaigns conducted by a Portuguese banking institution in promoting term deposit subscriptions. I plan to utilize machine learning classifiers—specifically K-Nearest Neighbors, Logistic Regression, Decision Trees, and Support Vector Machines—to build predictive models that accurately identify which clients are likely to subscribe to a term deposit based on their demographic, social, and economic features.\n",
    "\n",
    "This analysis will enable the bank to optimize its marketing strategies, enhance customer targeting, and improve overall campaign efficiency. Ultimately, the insights derived from the predictive models can help the bank increase subscription rates for term deposits, thereby boosting profitability and customer engagement.\n",
    "\n",
    "Key Goals:\n",
    "Identify significant features that influence client subscription decisions.\n",
    "Compare the performance of various classification algorithms to determine the most effective approach for predicting term deposit subscriptions.\n",
    "Provide actionable recommendations based on my findings to refine future marketing strategies and campaigns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Engineering Features\n",
    "\n",
    "Now that you understand your business objective, we will build a basic model to get started.  Before we can do this, we must work to encode the data.  Using just the bank information features, prepare the features and target column for modeling with appropriate encoding and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       age  job_blue-collar  job_entrepreneur  job_housemaid  job_management  \\\n",
      "12556   40             True             False          False           False   \n",
      "35451   31            False             False          False           False   \n",
      "30592   59            False             False          False           False   \n",
      "17914   43            False             False           True           False   \n",
      "3315    39            False             False          False           False   \n",
      "\n",
      "       job_retired  job_self-employed  job_services  job_student  \\\n",
      "12556        False              False         False        False   \n",
      "35451        False              False         False        False   \n",
      "30592         True              False         False        False   \n",
      "17914        False              False         False        False   \n",
      "3315         False              False         False        False   \n",
      "\n",
      "       job_technician  ...  education_illiterate  \\\n",
      "12556           False  ...                 False   \n",
      "35451           False  ...                 False   \n",
      "30592           False  ...                 False   \n",
      "17914           False  ...                 False   \n",
      "3315            False  ...                 False   \n",
      "\n",
      "       education_professional.course  education_university.degree  \\\n",
      "12556                          False                        False   \n",
      "35451                          False                         True   \n",
      "30592                          False                        False   \n",
      "17914                          False                        False   \n",
      "3315                           False                        False   \n",
      "\n",
      "       education_unknown  default_unknown  default_yes  housing_unknown  \\\n",
      "12556              False             True        False            False   \n",
      "35451              False            False        False            False   \n",
      "30592              False            False        False            False   \n",
      "17914              False            False        False            False   \n",
      "3315               False             True        False            False   \n",
      "\n",
      "       housing_yes  loan_unknown  loan_yes  \n",
      "12556         True         False     False  \n",
      "35451        False         False     False  \n",
      "30592        False         False     False  \n",
      "17914         True         False     False  \n",
      "3315         False         False     False  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/bank-additional-full.csv', sep=';')\n",
    "\n",
    "# Subset of bank client information features\n",
    "bank_info_columns = ['age', 'job', 'marital', 'education', 'default', 'housing', 'loan']\n",
    "X = df[bank_info_columns]\n",
    "y = df['y']  # Target column\n",
    "\n",
    "# One-hot encoding for categorical features\n",
    "X_encoded = pd.get_dummies(X, columns=['job', 'marital', 'education', 'default', 'housing', 'loan'], drop_first=True)\n",
    "\n",
    "# Encode target column (y: 'yes' = 1, 'no' = 0)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split into training and testing datasets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the transformed data\n",
    "print(X_train.head())\n",
    "print(y_train[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: Train/Test Split\n",
    "\n",
    "With your data prepared, split it into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (32950, 53)\n",
      "Test features shape: (8238, 53)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/bank-additional-full.csv', sep=';')\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Separate the features (X) and the target (y)\n",
    "X = df_clean.drop('y', axis=1)  # Drop the target column from features\n",
    "y = df_clean['y']  # Target column\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "numeric_cols = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "\n",
    "# Step 1: Encoding categorical variables and scaling numerical variables\n",
    "# Use OneHotEncoder for the categorical features and StandardScaler for numeric features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),  # Apply StandardScaler to numerical columns\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_cols)  # Apply OneHotEncoder to categorical columns\n",
    "    ])\n",
    "\n",
    "# Step 2: Splitting data into train and test sets\n",
    "# Set test size to 20% of the data, and random_state to ensure reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit and transform the training data, and only transform the test data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Optional: Convert the transformed data back to a DataFrame if needed\n",
    "# Get feature names after transformation for better understanding\n",
    "ohe_columns = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "feature_names = numeric_cols + ohe_columns.tolist()\n",
    "\n",
    "X_train_transformed = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
    "X_test_transformed = pd.DataFrame(X_test_transformed, columns=feature_names)\n",
    "\n",
    "print(\"Training features shape:\", X_train_transformed.shape)\n",
    "print(\"Test features shape:\", X_test_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7: A Baseline Model\n",
    "\n",
    "Before we build our first model, we want to establish a baseline.  What is the baseline performance that our classifier should aim to beat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "no     36548\n",
      "yes     4640\n",
      "Name: count, dtype: int64\n",
      "The most frequent class is: no\n",
      "Baseline accuracy (predicting only the most frequent class): 0.8873\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check the distribution of the target variable 'y'\n",
    "print(y.value_counts())\n",
    "\n",
    "# Calculate the proportion of the most frequent class\n",
    "most_frequent_class = y.mode()[0]  # Get the most frequent class (mode of the target variable)\n",
    "baseline_accuracy = np.mean(y == most_frequent_class)  # Proportion of the dataset with the most frequent class\n",
    "\n",
    "print(f\"The most frequent class is: {most_frequent_class}\")\n",
    "print(f\"Baseline accuracy (predicting only the most frequent class): {baseline_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8: A Simple Model\n",
    "\n",
    "Use Logistic Regression to build a basic model on your data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy: 0.9111\n",
      "Confusion Matrix:\n",
      "[[7105  198]\n",
      " [ 534  401]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.93      0.97      0.95      7303\n",
      "         yes       0.67      0.43      0.52       935\n",
      "\n",
      "    accuracy                           0.91      8238\n",
      "   macro avg       0.80      0.70      0.74      8238\n",
      "weighted avg       0.90      0.91      0.90      8238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "log_reg.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = log_reg.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Logistic Regression Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9: Score the Model\n",
    "\n",
    "What is the accuracy of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy: 0.9111\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f\"Logistic Regression Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10: Model Comparisons\n",
    "\n",
    "Now, we aim to compare the performance of the Logistic Regression model to our KNN algorithm, Decision Tree, and SVM models.  Using the default settings for each of the models, fit and score each.  Also, be sure to compare the fit time of each of the models.  Present your findings in a `DataFrame` similar to that below:\n",
    "\n",
    "| Model | Train Time | Train Accuracy | Test Accuracy |\n",
    "| ----- | ---------- | -------------  | -----------   |\n",
    "|     |    |.     |.     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "no     36548\n",
      "yes     4640\n",
      "Name: count, dtype: int64\n",
      "The most frequent class is: no\n",
      "Baseline accuracy (predicting only the most frequent class): 0.8873\n",
      "Logistic Regression Model Accuracy: 0.9113\n",
      "Confusion Matrix:\n",
      "[[7104  199]\n",
      " [ 532  403]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.93      0.97      0.95      7303\n",
      "         yes       0.67      0.43      0.52       935\n",
      "\n",
      "    accuracy                           0.91      8238\n",
      "   macro avg       0.80      0.70      0.74      8238\n",
      "weighted avg       0.90      0.91      0.90      8238\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ln/yypgv5k13fx2w7hgcd8j84d00000gn/T/ipykernel_44753/2225288367.py:103: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, current_result], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Train Time  Train Accuracy  Test Accuracy\n",
      "0  Logistic Regression    0.240136        0.911927       0.911265\n",
      "1                  KNN    0.023439        0.927709       0.900704\n",
      "2        Decision Tree    0.180388        1.000000       0.886380\n",
      "3                  SVM    9.270514        0.922671       0.911750\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/bank-additional-full.csv', sep=';')\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Separate the features (X) and the target (y)\n",
    "X = df_clean.drop('y', axis=1)  # Drop the target column from features\n",
    "y = df_clean['y']  # Target column\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "numeric_cols = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "\n",
    "# Step 1: Encoding categorical variables and scaling numerical variables\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),  # Apply StandardScaler to numerical columns\n",
    "        ('cat', OneHotEncoder(), categorical_cols)  # Apply OneHotEncoder to categorical columns\n",
    "    ])\n",
    "\n",
    "# Step 2: Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit and transform the training data, and only transform the test data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Check the distribution of the target variable 'y'\n",
    "print(y.value_counts())\n",
    "\n",
    "# Calculate the proportion of the most frequent class\n",
    "most_frequent_class = y.mode()[0]  # Get the most frequent class (mode of the target variable)\n",
    "baseline_accuracy = np.mean(y == most_frequent_class)  # Proportion of the dataset with the most frequent class\n",
    "\n",
    "print(f\"The most frequent class is: {most_frequent_class}\")\n",
    "print(f\"Baseline accuracy (predicting only the most frequent class): {baseline_accuracy:.4f}\")\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the model on the transformed training data\n",
    "log_reg.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Make predictions on the transformed test data\n",
    "y_pred = log_reg.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Logistic Regression Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Prepare to compare different models\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"SVM\": SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# DataFrame to hold results\n",
    "results = pd.DataFrame(columns=[\"Model\", \"Train Time\", \"Train Accuracy\", \"Test Accuracy\"])\n",
    "\n",
    "# Fit and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    start_time = time.time()  # Start time\n",
    "    model.fit(X_train_transformed, y_train)  # Fit model\n",
    "    fit_time = time.time() - start_time  # End time\n",
    "    \n",
    "    # Compute accuracies\n",
    "    train_accuracy = accuracy_score(y_train, model.predict(X_train_transformed))\n",
    "    test_accuracy = accuracy_score(y_test, model.predict(X_test_transformed))\n",
    "    \n",
    "    # Create a temporary DataFrame for the current model's results\n",
    "    current_result = pd.DataFrame({\n",
    "        \"Model\": [model_name],\n",
    "        \"Train Time\": [fit_time],\n",
    "        \"Train Accuracy\": [train_accuracy],\n",
    "        \"Test Accuracy\": [test_accuracy]\n",
    "    })\n",
    "    \n",
    "    # Append the current result to the results DataFrame\n",
    "    results = pd.concat([results, current_result], ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ln/yypgv5k13fx2w7hgcd8j84d00000gn/T/ipykernel_44753/2447433343.py:72: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, current_result], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Train Time  Train Accuracy  Test Accuracy\n",
      "0  Logistic Regression    0.419235        0.911927       0.911265\n",
      "1                  KNN    0.023599        0.927709       0.900704\n",
      "2        Decision Tree    0.174611        1.000000       0.886380\n",
      "3                  SVM    9.606932        0.922671       0.911750\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/bank-additional-full.csv', sep=';')\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Separate the features (X) and the target (y)\n",
    "X = df_clean.drop('y', axis=1)  # Drop the target column from features\n",
    "y = df_clean['y']  # Target column\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "numeric_cols = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "\n",
    "# Step 1: Encoding categorical variables and scaling numerical variables\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),  # Apply StandardScaler to numerical columns\n",
    "        ('cat', OneHotEncoder(), categorical_cols)  # Apply OneHotEncoder to categorical columns\n",
    "    ])\n",
    "\n",
    "# Step 2: Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit and transform the training data, and only transform the test data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Prepare to compare different models\n",
    "# Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"SVM\": SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# DataFrame to hold results\n",
    "results = pd.DataFrame(columns=[\"Model\", \"Train Time\", \"Train Accuracy\", \"Test Accuracy\"])\n",
    "\n",
    "# Fit and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    start_time = time.time()  # Start time\n",
    "    model.fit(X_train_transformed, y_train)  # Fit model\n",
    "    fit_time = time.time() - start_time  # End time\n",
    "    \n",
    "    # Compute accuracies\n",
    "    train_accuracy = accuracy_score(y_train, model.predict(X_train_transformed))\n",
    "    test_accuracy = accuracy_score(y_test, model.predict(X_test_transformed))\n",
    "    \n",
    "    # Create a temporary DataFrame for the current model's results\n",
    "    current_result = pd.DataFrame({\n",
    "        \"Model\": [model_name],\n",
    "        \"Train Time\": [fit_time],\n",
    "        \"Train Accuracy\": [train_accuracy],\n",
    "        \"Test Accuracy\": [test_accuracy]\n",
    "    })\n",
    "    \n",
    "    # Append the current result to the results DataFrame\n",
    "    results = pd.concat([results, current_result], ignore_index=True)\n",
    "\n",
    "# Display results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11: Improving the Model\n",
    "\n",
    "Now that we have some basic models on the board, we want to try to improve these.  Below, we list a few things to explore in this pursuit.\n",
    "\n",
    "- More feature engineering and exploration.  For example, should we keep the gender feature?  Why or why not?\n",
    "- Hyperparameter tuning and grid search.  All of our models have additional hyperparameters to tune and explore.  For example the number of neighbors in KNN or the maximum depth of a Decision Tree.  \n",
    "- Adjust your performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature Engineering:\n",
    "\n",
    "The code retains the existing features from the dataset, which include both categorical and numerical variables. However, it’s important to note that there is room for further enhancement. You might consider adjusting or adding new features based on insights gained from exploratory data analysis. For example, creating interaction terms or aggregating features could provide additional predictive power to the models.\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "\n",
    "For the K-Nearest Neighbors (KNN), Decision Tree, and Support Vector Machine (SVM) models, hyperparameter grids are established to explore the optimal settings for each algorithm. The code employs GridSearchCV, which systematically evaluates combinations of hyperparameters to identify the best-performing parameters for each model. Notably, Logistic Regression is excluded from this tuning process because it has fewer parameters that require adjustment, making it less complex in this regard.\n",
    "\n",
    "Performance Metrics:\n",
    "\n",
    "The code is structured to include accuracy metrics for model evaluation. It calculates and displays the training and test accuracy for each model. Additionally, a detailed classification report is printed for the best-performing SVM model, offering insights into precision, recall, and F1-score, which are crucial for understanding model performance beyond simple accuracy.\n",
    "\n",
    "Execution:\n",
    "\n",
    "To run this code successfully, ensure that you have all the required libraries installed, specifically pandas, numpy, and sklearn. Additionally, double-check that the dataset path is correctly specified in the pd.read_csv function to avoid file-related errors. Once everything is set up, you can execute the code in your Python environment to observe the results.\n",
    "\n",
    "This code structure is designed to be scalable, allowing for future enhancements and modifications. As you progress with your analysis, feel free to adjust the hyperparameters or incorporate additional feature engineering based on your findings during exploratory analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "no     36548\n",
      "yes     4640\n",
      "Name: count, dtype: int64\n",
      "The most frequent class is: no\n",
      "Baseline accuracy (predicting only the most frequent class): 0.8873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ln/yypgv5k13fx2w7hgcd8j84d00000gn/T/ipykernel_44753/1403827687.py:81: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, current_result], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Train Time  Train Accuracy  Test Accuracy\n",
      "0  Logistic Regression    0.216194        0.911927       0.911265\n",
      "1                  KNN    0.023711        0.927709       0.900704\n",
      "2        Decision Tree    0.173566        1.000000       0.886380\n",
      "3                  SVM    9.328113        0.922671       0.911750\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data/bank-additional-full.csv', sep=';')\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Separate the features (X) and the target (y)\n",
    "X = df_clean.drop('y', axis=1)  # Drop the target column from features\n",
    "y = df_clean['y']  # Target column\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "numeric_cols = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "\n",
    "# Step 1: Encoding categorical variables and scaling numerical variables\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),  # Apply StandardScaler to numerical columns\n",
    "        ('cat', OneHotEncoder(), categorical_cols)  # Apply OneHotEncoder to categorical columns\n",
    "    ])\n",
    "\n",
    "# Step 2: Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit and transform the training data, and only transform the test data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Check the distribution of the target variable 'y'\n",
    "print(y.value_counts())\n",
    "\n",
    "# Calculate the proportion of the most frequent class\n",
    "most_frequent_class = y.mode()[0]  # Get the most frequent class (mode of the target variable)\n",
    "baseline_accuracy = np.mean(y == most_frequent_class)  # Proportion of the dataset with the most frequent class\n",
    "\n",
    "print(f\"The most frequent class is: {most_frequent_class}\")\n",
    "print(f\"Baseline accuracy (predicting only the most frequent class): {baseline_accuracy:.4f}\")\n",
    "\n",
    "# Initialize models for comparison\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"SVM\": SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# DataFrame to hold results\n",
    "results = pd.DataFrame(columns=[\"Model\", \"Train Time\", \"Train Accuracy\", \"Test Accuracy\"])\n",
    "\n",
    "# Fit and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    start_time = time.time()  # Start time\n",
    "    model.fit(X_train_transformed, y_train)  # Fit model\n",
    "    fit_time = time.time() - start_time  # End time\n",
    "    \n",
    "    # Compute accuracies\n",
    "    train_accuracy = accuracy_score(y_train, model.predict(X_train_transformed))\n",
    "    test_accuracy = accuracy_score(y_test, model.predict(X_test_transformed))\n",
    "    \n",
    "    # Create a temporary DataFrame for the current model's results\n",
    "    current_result = pd.DataFrame({\n",
    "        \"Model\": [model_name],\n",
    "        \"Train Time\": [fit_time],\n",
    "        \"Train Accuracy\": [train_accuracy],\n",
    "        \"Test Accuracy\": [test_accuracy]\n",
    "    })\n",
    "    \n",
    "    # Append the current result to the results DataFrame\n",
    "    results = pd.concat([results, current_result], ignore_index=True)\n",
    "\n",
    "# Print results for baseline models\n",
    "print(results)\n",
    "\n",
    "# Step 3: Hyperparameter tuning for each model\n",
    "# Define parameter grids for hyperparameter tuning\n",
    "param_grids = {\n",
    "    \"KNN\": {'n_neighbors': [3, 5, 10, 15], 'weights': ['uniform', 'distance']},\n",
    "    \"Decision Tree\": {\n",
    "        'max_depth': [None, 5, 10, 15, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for each model (excluding Logistic Regression for simplicity)\n",
    "for model_name in param_grids.keys():\n",
    "    model = models[model_name]\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "    start_time = time.time()  # Start time for GridSearch\n",
    "    grid_search.fit(X_train_transformed, y_train)\n",
    "    fit_time = time.time() - start_time  # End time for GridSearch\n",
    "    \n",
    "    # Best parameters and scores\n",
    "    best_model = grid_search.best_estimator_\n",
    "    train_accuracy = accuracy_score(y_train, best_model.predict(X_train_transformed))\n",
    "    test_accuracy = accuracy_score(y_test, best_model.predict(X_test_transformed))\n",
    "    \n",
    "    # Append the results to the DataFrame\n",
    "    current_result = pd.DataFrame({\n",
    "        \"Model\": [f\"{model_name} (Tuned)\"],\n",
    "        \"Train Time\": [fit_time],\n",
    "        \"Train Accuracy\": [train_accuracy],\n",
    "        \"Test Accuracy\": [test_accuracy]\n",
    "    })\n",
    "    \n",
    "    results = pd.concat([results, current_result], ignore_index=True)\n",
    "\n",
    "# Display final results including tuned models\n",
    "print(\"\\nModel Comparison Results:\")\n",
    "print(results)\n",
    "\n",
    "# Print classification report for the best model\n",
    "print(\"\\nClassification Report for the best SVM model:\")\n",
    "y_pred_best_svm = best_model.predict(X_test_transformed)\n",
    "print(classification_report(y_test, y_pred_best_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
